{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SHAPE (1000,)\n",
      "TARGET rec.autos\n"
     ]
    }
   ],
   "source": [
    "import kmapper as km\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='train')\n",
    "X, y, target_names = np.array(newsgroups.data[:1000]), np.array(newsgroups.target[:1000]), np.array(newsgroups.target_names[:1000])\n",
    "print(\"SAMPLE\",X[0])\n",
    "print(\"SHAPE\",X.shape)\n",
    "print(\"TARGET\",target_names[y[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 0]\n",
      " [0 1 0 0 0 1 1]\n",
      " [1 1 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "tags = [\n",
    "  \"python, tools\",\n",
    "  \"linux, tools, ubuntu\",\n",
    "  \"distributed systems, linux, networking, tools\",\n",
    "]\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "data = vec.fit_transform(X).toarray()\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..Projecting data using: sum\n",
      "\n",
      "..Scaling with: MinMaxScaler(copy=True, feature_range=(0, 1))\n",
      "\n",
      "Mapping on data shaped (3, 1) using lens shaped (3, 1)\n",
      "\n",
      "Minimal points in hypercube before clustering: 3\n",
      "Creating 10 hypercubes.\n",
      "There are 1 points in cube_0 / 10\n",
      "Cube_0 is empty.\n",
      "\n",
      "There are 0 points in cube_1 / 10\n",
      "Cube_1 is empty.\n",
      "\n",
      "There are 0 points in cube_2 / 10\n",
      "Cube_2 is empty.\n",
      "\n",
      "There are 1 points in cube_3 / 10\n",
      "Cube_3 is empty.\n",
      "\n",
      "There are 0 points in cube_4 / 10\n",
      "Cube_4 is empty.\n",
      "\n",
      "There are 0 points in cube_5 / 10\n",
      "Cube_5 is empty.\n",
      "\n",
      "There are 0 points in cube_6 / 10\n",
      "Cube_6 is empty.\n",
      "\n",
      "There are 0 points in cube_7 / 10\n",
      "Cube_7 is empty.\n",
      "\n",
      "There are 0 points in cube_8 / 10\n",
      "Cube_8 is empty.\n",
      "\n",
      "There are 1 points in cube_9 / 10\n",
      "Cube_9 is empty.\n",
      "\n",
      "\n",
      "Created 0 edges and 0 nodes in 0:00:00.016002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maitreyi_kv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "p_x = mapper.fit_transform(data)\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "graph = mapper.map(p_x, inverse_X=None, clusterer = cluster.AgglomerativeClustering(n_clusters=3,\n",
    "                                                                                   linkage=\"complete\",\n",
    "                                                                                   affinity=\"cosine\"),\n",
    "                  overlap_perc=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE (1000, 999)\n",
      "FEATURE NAMES SAMPLE ['00', '000', '01', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '1990', '1992', '1993', '1993apr15', '1993apr20', '20', '2000', '21', '22', '23', '24', '25', '250', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '39', '40', '408', '41', '42', '43', '44', '45', '50', '51', '55', '56', '58', '60', '70', '75', '80', '90', '93', 'able', 'ac', 'ac uk', 'accept', 'access', 'according', 'acs', 'act', 'action', 'actions', 'actually', 'ad', 'add', 'addition', 'address', 'advance', 'advice', 'age', 'ago', 'agree', 'air', 'al', 'alt', 'america', 'american', 'andrew', 'answer', 'answers', 'anti', 'anybody', 'apparently', 'appears', 'apple', 'application', 'applications', 'applied', 'appreciate', 'appreciated', 'appropriate', 'apr', 'apr 1993', 'apr 93', 'april', 'area', 'aren', 'argument', 'article', 'article 1993apr15', 'article 1993apr20', 'article apr', 'ask', 'asked', 'asking', 'assume', 'assuming', 'atheists', 'athos', 'athos rutgers', 'athos rutgers edu', 'att', 'att com', 'attack', 'attention', 'au', 'austin', 'available', 'avoid', 'aware', 'away', 'bad', 'base', 'based', 'basic', 'basically', 'basis', 'bbs', 'begin', 'behavior', 'belief', 'believe', 'bell', 'best', 'better', 'bible', 'big', 'bike', 'bit', 'bitnet', 'black', 'blue', 'board', 'bob', 'body', 'book', 'boston', 'bought', 'box', 'break', 'brian', 'bring', 'brought', 'btw', 'build', 'building', 'built', 'business', 'buy', 'ca', 'ca lines', 'california', 'called', 'calls', 'caltech', 'came', 'canada', 'car', 'card', 'care', 'carry', 'cars', 'case', 'cause', 'caused', 'cc', 'center', 'certain', 'certainly', 'change', 'changed', 'cheap', 'check', 'chicago', 'children', 'chip', 'choice', 'chris', 'christ', 'christian', 'christianity', 'christians', 'church', 'city', 'claim', 'class', 'clear', 'clearly', 'cleveland', 'clipper', 'close', 'closed', 'cmu', 'cmu edu', 'code', 'college', 'color', 'colorado', 'columbia', 'com', 'com mark', 'com organization', 'come', 'comes', 'coming', 'command', 'comments', 'common', 'communications', 'company', 'complete', 'completely', 'computer', 'computer science', 'computing', 'condition', 'consider', 'considered', 'contact', 'context', 'continue', 'control', 'copy', 'corp', 'corporation', 'correct', 'correctly', 'cost', 'costs', 'couldn', 'country', 'couple', 'course', 'create', 'created', 'cross', 'cs', 'cso', 'cso uiuc', 'cso uiuc edu', 'current', 'currently', 'cut', 'cwru', 'cwru edu', 'damn', 'data', 'date', 'dave', 'david', 'day', 'days', 'dead', 'deal', 'death', 'decided', 'defense', 'definition', 'deleted', 'department', 'dept', 'details', 'development', 'did', 'didn', 'difference', 'different', 'difficult', 'digital', 'directly', 'disclaimer', 'discussion', 'disk', 'display', 'distribution', 'distribution na', 'distribution na lines', 'distribution usa', 'distribution usa lines', 'distribution world', 'distribution world nntp', 'distribution world organization', 'division', 'dod', 'does', 'does know', 'doesn', 'doing', 'don', 'don know', 'don think', 'don want', 'dos', 'doubt', 'doug', 'dr', 'drive', 'driver', 'drivers', 'early', 'earth', 'easily', 'east', 'easy', 'ed', 'edu', 'edu article', 'edu au', 'edu david', 'edu organization', 'edu organization university', 'edu reply', 'edu subject', 'edu writes', 'effect', 'email', 'encryption', 'end', 'engine', 'engineering', 'entire', 'error', 'escape', 'especially', 'europe', 'event', 'events', 'evidence', 'exactly', 'example', 'excellent', 'excuse', 'exist', 'exists', 'expect', 'expected', 'experience', 'explain', 'expressed', 'face', 'fact', 'failed', 'fair', 'faith', 'fall', 'family', 'far', 'fast', 'faster', 'fax', 'feel', 'figure', 'file', 'files', 'final', 'finally', 'fine', 'folks', 'follow', 'following', 'force', 'forget', 'form', 'frank', 'free', 'friend', 'ftp', 'future', 'game', 'games', 'gas', 'gave', 'general', 'george', 'germany', 'gets', 'getting', 'given', 'gives', 'giving', 'gmt', 'god', 'goes', 'going', 'good']\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                      strip_accents=\"unicode\",\n",
    "                      stop_words=\"english\",\n",
    "                      ngram_range=(1,3),\n",
    "                      max_df=0.97,\n",
    "                      min_df=0.02)\n",
    "\n",
    "interpretable_inverse_X = vec.fit_transform(X).toarray()\n",
    "interpretable_inverse_X_names = vec.get_feature_names()\n",
    "\n",
    "print(\"SHAPE\", interpretable_inverse_X.shape)\n",
    "print(\"FEATURE NAMES SAMPLE\", interpretable_inverse_X_names[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "..Projecting data using: [TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.83, max_features=None, min_df=0.05,\n",
      "        ngram_range=(1, 6), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None), TruncatedSVD(algorithm='randomized', n_components=100, n_iter=5,\n",
      "       random_state=1729, tol=0.0), Isomap(eigen_solver='auto', max_iter=None, n_components=2, n_jobs=-1,\n",
      "    n_neighbors=5, neighbors_algorithm='auto', path_method='auto', tol=0)]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5ac99241f928>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                 Isomap(n_components=2,\n\u001b[0;32m     11\u001b[0m                        n_jobs=-1)],\n\u001b[1;32m---> 12\u001b[1;33m     scaler=[None, None, MinMaxScaler()])\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SHAPE\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprojected_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\maitreyi_kv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\kmapper\\kmapper.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, projection, scaler, distance_matrix)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n..Projecting data using: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# Scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "mapper = km.KeplerMapper(verbose=2)\n",
    "\n",
    "projected_X = mapper.fit_transform(X,\n",
    "    projection=[TfidfVectorizer(analyzer=\"char\",\n",
    "                                ngram_range=(1,6),\n",
    "                                max_df=0.83,\n",
    "                                min_df=0.05),\n",
    "                TruncatedSVD(n_components=100,\n",
    "                             random_state=1729),\n",
    "                Isomap(n_components=2,\n",
    "                       n_jobs=-1)],\n",
    "    scaler=[None, None, MinMaxScaler()])\n",
    "\n",
    "print(\"SHAPE\",projected_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
